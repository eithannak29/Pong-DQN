{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "writer = SummaryWriter(f\"runs/pong_dqn_{current_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 1000000\n",
    "TARGET_UPDATE = 1000\n",
    "MEMORY_SIZE = 10000\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    obs = obs[35:195]  # Crop\n",
    "    obs = cv2.resize(obs, (84, 84))  # Resize\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    _, obs = cv2.threshold(obs, 1, 255, cv2.THRESH_BINARY)  # Binary\n",
    "    return obs / 255.0  # Normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.out = nn.Linear(512, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_space)\n",
    "    else:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            return torch.argmax(policy_net(state)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "\n",
    "    # Conversion en tenseurs PyTorch\n",
    "    batch_state = torch.cat([torch.tensor(s, dtype=torch.float32).unsqueeze(0) for s in batch_state])\n",
    "    batch_action = torch.tensor(batch_action)\n",
    "    batch_reward = torch.tensor(batch_reward)\n",
    "    batch_next_state = torch.cat([torch.tensor(s, dtype=torch.float32).unsqueeze(0) for s in batch_next_state])\n",
    "    batch_done = torch.tensor(batch_done, dtype=torch.bool)\n",
    "\n",
    "    current_q_values = policy_net(batch_state).gather(1, batch_action.unsqueeze(1))\n",
    "    next_q_values = target_net(batch_next_state).max(1)[0].detach()\n",
    "    expected_q_values = batch_reward + (GAMMA * next_q_values) * (~batch_done)\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\", difficulty=1, render_mode=\"human\")\n",
    "policy_net = DQN(env.action_space.n)\n",
    "target_net = DQN(env.action_space.n)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 0/5000 [00:00<?, ?it/s]2024-11-01 12:07:45.281 python[43200:13004012] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-11-01 12:07:45.281 python[43200:13004012] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "/var/folders/nm/xmbyqkhd1wjg9j848np2w3gr0000gn/T/ipykernel_43200/2429118745.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch_state = torch.cat([torch.tensor(s, dtype=torch.float32).unsqueeze(0) for s in batch_state])\n",
      "Training Episodes:   0%|          | 1/5000 [13:05<1090:35:56, 785.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0, Total Reward: -20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 1/5000 [28:46<2398:02:13, 1726.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3209\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'ndim'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m policy_net\u001b[38;5;241m.\u001b[39mnamed_parameters():\n\u001b[1;32m     38\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mrequires_grad:\n\u001b[0;32m---> 39\u001b[0m       \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m           writer\u001b[38;5;241m.\u001b[39madd_histogram(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_gradients\u001b[39m\u001b[38;5;124m\"\u001b[39m, param\u001b[38;5;241m.\u001b[39mgrad, episode)\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/writer.py:526\u001b[0m, in \u001b[0;36mSummaryWriter.add_histogram\u001b[0;34m(self, tag, values, global_step, bins, walltime, max_bins)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(bins, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m bins \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    524\u001b[0m     bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefault_bins\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_file_writer()\u001b[38;5;241m.\u001b[39madd_summary(\n\u001b[0;32m--> 526\u001b[0m     \u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_bins\u001b[49m\u001b[43m)\u001b[49m, global_step, walltime\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:483\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(name, values, bins, max_bins)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Output a `Summary` protocol buffer with a histogram.\u001b[39;00m\n\u001b[1;32m    468\u001b[0m \n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03mThe generated\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;124;03m  buffer.\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    482\u001b[0m values \u001b[38;5;241m=\u001b[39m make_np(values)\n\u001b[0;32m--> 483\u001b[0m hist \u001b[38;5;241m=\u001b[39m \u001b[43mmake_histogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Summary(value\u001b[38;5;241m=\u001b[39m[Summary\u001b[38;5;241m.\u001b[39mValue(tag\u001b[38;5;241m=\u001b[39mname, histo\u001b[38;5;241m=\u001b[39mhist)])\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/torch/utils/tensorboard/summary.py:492\u001b[0m, in \u001b[0;36mmake_histogram\u001b[0;34m(values, bins, max_bins)\u001b[0m\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input has no element.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    491\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 492\u001b[0m counts, limits \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m num_bins \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(counts)\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_bins \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m num_bins \u001b[38;5;241m>\u001b[39m max_bins:\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/numpy/lib/histograms.py:780\u001b[0m, in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, density, weights)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;124;03mCompute the histogram of a dataset.\u001b[39;00m\n\u001b[1;32m    682\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m \n\u001b[1;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    778\u001b[0m a, weights \u001b[38;5;241m=\u001b[39m _ravel_and_check_weights(a, weights)\n\u001b[0;32m--> 780\u001b[0m bin_edges, uniform_bins \u001b[38;5;241m=\u001b[39m \u001b[43m_get_bin_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;66;03m# Histogram is an integer or a float array depending on the weights.\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/numpy/lib/histograms.py:428\u001b[0m, in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`bins` must be positive, when an integer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    426\u001b[0m     first_edge, last_edge \u001b[38;5;241m=\u001b[39m _get_outer_edges(a, \u001b[38;5;28mrange\u001b[39m)\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    429\u001b[0m     bin_edges \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(bins)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39many(bin_edges[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m bin_edges[\u001b[38;5;241m1\u001b[39m:]):\n",
      "File \u001b[0;32m~/Desktop/SCIA/S9/Reinforcement Learning/Pong-DQN/.venv/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3211\u001b[0m, in \u001b[0;36mndim\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m   3209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\u001b[38;5;241m.\u001b[39mndim\n\u001b[1;32m   3210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m-> 3211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mndim\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 5000\n",
    "epsilon = EPSILON_START\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "    obs, _ = env.reset()\n",
    "    state = preprocess_observation(obs)\n",
    "    state = np.stack([state] * 4, axis=0)\n",
    "    state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon, env.action_space.n)\n",
    "        next_obs, reward, done, truncated, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        next_state = preprocess_observation(next_obs)\n",
    "        next_state = np.concatenate((state[1:, :, :], np.expand_dims(next_state, 0)), axis=0)\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        # Optimisation du modèle et enregistrement de la perte\n",
    "        loss = optimize_model()\n",
    "        if loss is not None:\n",
    "            writer.add_scalar(\"Loss\", loss, global_step)\n",
    "        \n",
    "        if steps % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        steps += 1\n",
    "        global_step += 1  # Incrémente global_step pour chaque étape\n",
    "        epsilon = max(EPSILON_END, EPSILON_START - steps / EPSILON_DECAY)\n",
    "        \n",
    "        for name, param in policy_net.named_parameters():\n",
    "          if param.requires_grad:\n",
    "              writer.add_histogram(f\"{name}_weights\", param, episode)\n",
    "              if param.grad is not None:\n",
    "                  writer.add_histogram(f\"{name}_gradients\", param.grad, episode)\n",
    "        \n",
    "        writer.add_scalar(\"Epsilon\", epsilon, global_step)\n",
    "\n",
    "    writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    tqdm.write(f\"Episode {episode}, Total Reward: {total_reward}\")\n",
    "\n",
    "writer.close()  # Ferme le writer après l'entraînement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
