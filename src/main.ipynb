{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.99\n",
    "TARGET_UPDATE = 1000\n",
    "MAX_MEMORY_SIZE = 100\n",
    "MIN_MEMORY_SIZE = 80\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(image):\n",
    "    target_h =64  # Height after process \n",
    "    target_w = 80  # Widht after process\n",
    "    crop_dim = [20, 84, 0, 84] # Cut 20 px from top to get rid of the score table \n",
    "    frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # To grayscale\n",
    "    frame = frame[crop_dim[0]:crop_dim[1], crop_dim[2]:crop_dim[3]]  # Cut 20 px from top\n",
    "    frame = cv2.resize(frame, (target_h, target_w))  # Resize\n",
    "    frame = frame.reshape(target_h, target_w) / 255  # Normalize\n",
    "    return torch.tensor(frame, dtype=torch.float32).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.value_stream = nn.Linear(1536, 128)  # Value function\n",
    "        self.value_stream2 = nn.Linear(128, 1)  # Value function\n",
    "        self.advantage_stream = nn.Linear(1536, 128)  # Advantage function\n",
    "        self.advantage_stream2 = nn.Linear(128, action_space)  # Advantage function\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        value = self.value_stream2(F.leaky_relu(self.value_stream(x)))\n",
    "        advantage = self.advantage_stream2(F.leaky_relu(self.advantage_stream(x)))\n",
    "        q_values = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+unknown)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\", difficulty=1)\n",
    "policy_net = DQN(env.action_space.n).to(device)\n",
    "target_net = DQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=MAX_MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_space)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0) ## Add batch dimension\n",
    "            return torch.argmax(policy_net(state)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < MIN_MEMORY_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "\n",
    "    # Conversion en tenseurs PyTorch\n",
    "    batch_state = torch.cat(batch_state).unflatten(0,(BATCH_SIZE,-1)).to(device)\n",
    "    batch_action = torch.tensor(batch_action).to(device)\n",
    "    batch_reward = torch.tensor(batch_reward).to(device)\n",
    "    batch_next_state = torch.cat(batch_next_state).unflatten(0,(BATCH_SIZE,-1)).to(device)\n",
    "    batch_done = torch.tensor(batch_done, dtype=torch.bool).to(device)\n",
    "\n",
    "    current_q_values = policy_net(batch_state).gather(1, batch_action.unsqueeze(1)).to(device)\n",
    "    next_q_values = target_net(batch_next_state).max(1)[0].detach().to(device)\n",
    "    expected_q_values = batch_reward + (GAMMA * next_q_values) * (~batch_done).to(device)\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    num_episodes = 1000\n",
    "    epsilon = EPSILON_START\n",
    "    writer = SummaryWriter(f'runs/pong_dqn_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "    global_step = 0\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        state = torch.stack([state] * 4, axis=0)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, epsilon, env.action_space.n)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            done = done or truncated\n",
    "            total_reward += reward\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            # next_state = torch.cat((state[1:, :, :], next_state.unsqueeze(0)), dim=0)\n",
    "            next_state = torch.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            global_step += 1  # Incrémente global_step pour chaque étape\n",
    "\n",
    "            # Optimisation du modèle et enregistrement de la perte\n",
    "            loss = optimize_model()\n",
    "            if loss is not None:\n",
    "                writer.add_scalar(\"Loss\", loss, global_step)\n",
    "\n",
    "            if steps % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "                \n",
    "            if global_step % 1000 == 0:\n",
    "                if epsilon > EPSILON_END:\n",
    "                    epsilon *= EPSILON_DECAY\n",
    "                \n",
    "        writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    writer.close()  # Ferme le writer après l'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the line_profiler extension\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Profile the run_episode function\n",
    "%lprun -f run_train -f optimize_model run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
