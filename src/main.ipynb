{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.02\n",
    "EPSILON_DECAY = 1000000\n",
    "TARGET_UPDATE = 1000\n",
    "MEMORY_SIZE = 4\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(obs):\n",
    "    obs = obs[35:195]  # Crop\n",
    "    obs = cv2.resize(obs, (84, 84))  # Resize\n",
    "    obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    _, obs = cv2.threshold(obs, 1, 255, cv2.THRESH_BINARY)  # Binary\n",
    "    return torch.tensor(obs / 255.0, dtype=torch.float32).to(device=device) # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN model\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.out = nn.Linear(512, action_space)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.relu(self.fc(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"PongNoFrameskip-v4\", difficulty=1)\n",
    "policy_net = DQN(env.action_space.n).to(device)\n",
    "target_net = DQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = deque(maxlen=MEMORY_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_space)\n",
    "    else:\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0) ## Add batch dimension\n",
    "            return torch.argmax(policy_net(state)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
    "\n",
    "    # Conversion en tenseurs PyTorch\n",
    "    batch_state = torch.cat(batch_state).unflatten(0,(BATCH_SIZE,-1)).to(device)\n",
    "    batch_action = torch.tensor(batch_action).to(device)\n",
    "    batch_reward = torch.tensor(batch_reward).to(device)\n",
    "    batch_next_state = torch.cat(batch_next_state).unflatten(0,(BATCH_SIZE,-1)).to(device)\n",
    "    batch_done = torch.tensor(batch_done, dtype=torch.bool).to(device)\n",
    "\n",
    "    current_q_values = policy_net(batch_state).gather(1, batch_action.unsqueeze(1)).to(device)\n",
    "    next_q_values = target_net(batch_next_state).max(1)[0].detach().to(device)\n",
    "    expected_q_values = batch_reward + (GAMMA * next_q_values) * (~batch_done).to(device)\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(current_q_values, expected_q_values.unsqueeze(1))\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    num_episodes = 5000\n",
    "    epsilon = EPSILON_START\n",
    "    writer = SummaryWriter(f'runs/pong_dqn_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
    "    global_step = 0\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        state = torch.stack([state] * 4, axis=0)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, epsilon, env.action_space.n)\n",
    "            next_obs, reward, done, truncated, info = env.step(action)\n",
    "            done = done or truncated\n",
    "            total_reward += reward\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            next_state = torch.cat((state[1:, :, :], next_state.unsqueeze(0)), dim=0)\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            global_step += 1  # Incrémente global_step pour chaque étape\n",
    "\n",
    "            # Optimisation du modèle et enregistrement de la perte\n",
    "            loss = optimize_model()\n",
    "            if loss is not None:\n",
    "                writer.add_scalar(\"Loss\", loss, global_step)\n",
    "\n",
    "            if steps % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            epsilon = max(EPSILON_END, EPSILON_START - steps / EPSILON_DECAY)\n",
    "            # for name, param in policy_net.named_parameters():\n",
    "            #     if param.requires_grad:\n",
    "            #         writer.add_histogram(f\"{name}_weights\", param, episode)\n",
    "            #         if param.grad is not None:\n",
    "            #             writer.add_histogram(f\"{name}_gradients\", param.grad, episode)\n",
    "        writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    writer.close()  # Ferme le writer après l'entraînement\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:  19%|█▊        | 932/5000 [13:02<56:54,  1.19it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[131], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[130], line 20\u001b[0m, in \u001b[0;36mrun_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m done \u001b[38;5;129;01mor\u001b[39;00m truncated\n\u001b[1;32m     19\u001b[0m total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m---> 20\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((state[\u001b[38;5;241m1\u001b[39m:, :, :], next_state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     22\u001b[0m memory\u001b[38;5;241m.\u001b[39mappend((state, action, reward, next_state, done))\n",
      "Cell \u001b[0;32mIn[124], line 6\u001b[0m, in \u001b[0;36mpreprocess_observation\u001b[0;34m(obs)\u001b[0m\n\u001b[1;32m      4\u001b[0m obs \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(obs, cv2\u001b[38;5;241m.\u001b[39mCOLOR_RGB2GRAY)  \u001b[38;5;66;03m# Convert to grayscale\u001b[39;00m\n\u001b[1;32m      5\u001b[0m _, obs \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mthreshold(obs, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY)  \u001b[38;5;66;03m# Binary\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m255.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Episodes:   0%|          | 2/5000 [00:05<3:59:34,  2.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** KeyboardInterrupt exception caught in code being profiled."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 1e-09 s\n",
      "\n",
      "Total time: 5.70917 s\n",
      "File: /tmp/ipykernel_259017/3695827517.py\n",
      "Function: run_train at line 1\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     1                                           def run_train():\n",
      "     2         1        956.0    956.0      0.0      num_episodes = 5000\n",
      "     3         1        270.0    270.0      0.0      epsilon = EPSILON_START\n",
      "     4         1    2662630.0    3e+06      0.0      writer = SummaryWriter(f'runs/pong_dqn_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}')\n",
      "     5         1        187.0    187.0      0.0      global_step = 0\n",
      "     6         3    2686663.0 895554.3      0.0      for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
      "     7         3    8358930.0    3e+06      0.1          obs, _ = env.reset()\n",
      "     8         3     664733.0 221577.7      0.0          state = preprocess_observation(obs)\n",
      "     9         3     134105.0  44701.7      0.0          state = np.stack([state] * 4, axis=0)\n",
      "    10         3    8363863.0    3e+06      0.1          state = torch.tensor(state, dtype=torch.float32).to(device)\n",
      "    11                                           \n",
      "    12         3       1855.0    618.3      0.0          total_reward = 0\n",
      "    13         3        441.0    147.0      0.0          done = False\n",
      "    14         3        351.0    117.0      0.0          steps = 0\n",
      "    15                                           \n",
      "    16      8491    2207592.0    260.0      0.0          while not done:\n",
      "    17      8489   64667705.0   7617.8      1.1              action = select_action(state, epsilon, env.action_space.n)\n",
      "    18      8489  802301727.0  94510.7     14.1              next_obs, reward, done, truncated, info = env.step(action)\n",
      "    19      8489    1356606.0    159.8      0.0              done = done or truncated\n",
      "    20      8489    1468196.0    173.0      0.0              total_reward += reward\n",
      "    21      8489 1020240947.0 120183.9     17.9              next_state = preprocess_observation(next_obs)\n",
      "    22     16976  116971688.0   6890.4      2.0              next_state = np.concatenate(\n",
      "    23      8488 2275473348.0 268081.2     39.9                  (state[1:, :, :].cpu().numpy(), np.expand_dims(next_state, 0)), axis=0\n",
      "    24                                                       )\n",
      "    25      8488  993195609.0 117011.7     17.4              next_state = torch.tensor(next_state, dtype=torch.float32).to(device)\n",
      "    26      8488   18760749.0   2210.3      0.3              memory.append((state, action, reward, next_state, done))\n",
      "    27      8488    1174415.0    138.4      0.0              state = next_state\n",
      "    28      8488    2405511.0    283.4      0.0              steps += 1\n",
      "    29      8488    1322484.0    155.8      0.0              global_step += 1  # Incrémente global_step pour chaque étape\n",
      "    30                                           \n",
      "    31                                                       # Optimisation du modèle et enregistrement de la perte\n",
      "    32      8488    8083125.0    952.3      0.1              loss = optimize_model()\n",
      "    33      8488    1469570.0    173.1      0.0              if loss is not None:\n",
      "    34                                                           writer.add_scalar(\"Loss\", loss, global_step)\n",
      "    35                                           \n",
      "    36      8488    3490352.0    411.2      0.1              if steps % TARGET_UPDATE == 0:\n",
      "    37         6    1816621.0 302770.2      0.0                  target_net.load_state_dict(policy_net.state_dict())\n",
      "    38                                           \n",
      "    39      8488    9276644.0   1092.9      0.2              epsilon = max(EPSILON_END, EPSILON_START - steps / EPSILON_DECAY)\n",
      "    40                                                       # for name, param in policy_net.named_parameters():\n",
      "    41                                                       #     if param.requires_grad:\n",
      "    42                                                       #         writer.add_histogram(f\"{name}_weights\", param, episode)\n",
      "    43                                                       #         if param.grad is not None:\n",
      "    44                                                       #             writer.add_histogram(f\"{name}_gradients\", param.grad, episode)\n",
      "    45      8488  360580065.0  42481.2      6.3              writer.add_scalar(\"Epsilon\", epsilon, global_step)\n",
      "    46         2      33194.0  16597.0      0.0          writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
      "    47                                               writer.close()  # Ferme le writer après l'entraînement"
     ]
    }
   ],
   "source": [
    "# Load the line_profiler extension\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Profile the run_episode function\n",
    "%lprun -f run_train run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
