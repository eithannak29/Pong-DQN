{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"PongDeterministic-v4\"\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.97\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.99\n",
    "TARGET_UPDATE = 1000\n",
    "MAX_MEMORY_SIZE = 25000\n",
    "MIN_MEMORY_SIZE = 24000\n",
    "LEARNING_RATE = 0.00025\n",
    "CHECKPOINT_PATH = \"./checkpoints\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualDQN(nn.Module):\n",
    "    def __init__(self, action_space: int):\n",
    "        super(DualDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.advantage_stream = nn.Linear(1536, 128)\n",
    "        self.advantage_stream2 = nn.Linear(128, action_space)\n",
    "        self.value_stream = nn.Linear(1536, 128)\n",
    "        self.value_stream2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.tensor:\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        value = self.value_stream2(F.leaky_relu(self.value_stream(x)))\n",
    "        advantage = self.advantage_stream2(F.leaky_relu(self.advantage_stream(x)))\n",
    "        q_values = value + (advantage - advantage.mean())\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(image: np.ndarray) -> torch.tensor:\n",
    "    target_h = 80\n",
    "    target_w = 64\n",
    "    crop_dim = [20, image.shape[0], 0, image.shape[1]]\n",
    "    frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame[crop_dim[0] : crop_dim[1], crop_dim[2] : crop_dim[3]]\n",
    "    frame = cv2.resize(frame, (target_w, target_h))\n",
    "    frame = frame.reshape(target_w, target_h) / 255\n",
    "    return torch.tensor(frame, dtype=torch.float32).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model(\n",
    "    memory: deque,\n",
    "    policy_net: DualDQN,\n",
    "    target_net: DualDQN,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    ") -> float:\n",
    "    if len(memory) < MIN_MEMORY_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "        *transitions\n",
    "    )\n",
    "\n",
    "    batch_state = torch.cat(batch_state).unflatten(0, (BATCH_SIZE, -1)).to(device)\n",
    "    batch_action = torch.tensor(batch_action).to(device)\n",
    "    batch_reward = torch.tensor(batch_reward).to(device)\n",
    "    batch_next_state = (\n",
    "        torch.cat(batch_next_state).unflatten(0, (BATCH_SIZE, -1)).to(device)\n",
    "    )\n",
    "    batch_done = torch.tensor(batch_done, dtype=torch.bool).to(device)\n",
    "\n",
    "    current_q_values = (\n",
    "        policy_net(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "    )\n",
    "    next_q_values = policy_net(batch_next_state)\n",
    "    next_target_q_values = (\n",
    "        target_net(batch_next_state)\n",
    "        .gather(1, next_q_values.max(1)[1].unsqueeze(1))\n",
    "        .squeeze(1)\n",
    "    )\n",
    "    expected_q_values = batch_reward + (GAMMA * next_target_q_values) * (~batch_done)\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(current_q_values, expected_q_values.detach())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(\n",
    "    state: torch.tensor, epsilon: float, action_space: int, policy_net: DualDQN\n",
    ") -> int:\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_space)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0)\n",
    "            return torch.argmax(policy_net(state)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_version(path: str = CHECKPOINT_PATH) -> Optional[str]:\n",
    "    versions = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    return max(versions) if versions else None\n",
    "\n",
    "\n",
    "def save_model_and_hyperparameter(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epsilon: float,\n",
    "    episode: int,\n",
    "    path: str = CHECKPOINT_PATH,\n",
    "    version: Optional[str] = None,\n",
    ") -> str:\n",
    "    if version is None:\n",
    "        version = datetime.now().strftime(\"pong_dqn_%Y-%m-%d_%H-%M-%S\")\n",
    "        print(f\"Version: {version}\")\n",
    "    version_path = os.path.join(path, version)\n",
    "    os.makedirs(version_path, exist_ok=True)\n",
    "\n",
    "    model_path = os.path.join(version_path, f\"pong-policy-net-{episode}.pt\")\n",
    "    optimizer_path = os.path.join(version_path, f\"pong-optimizer-{episode}.pt\")\n",
    "    epsilon_path = os.path.join(version_path, f\"epsilon-{episode}.json\")\n",
    "\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    torch.save(optimizer.state_dict(), optimizer_path)\n",
    "    with open(epsilon_path, \"w\") as f:\n",
    "        f.write(json.dumps({\"epsilon\": epsilon}))\n",
    "    return version\n",
    "\n",
    "\n",
    "def load_model_and_hyperparameter(\n",
    "    model: nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epsilon: float,\n",
    "    episode: int,\n",
    "    path: str = CHECKPOINT_PATH,\n",
    "    version: Optional[str] = None,\n",
    ") -> tuple[nn.Module, torch.optim.Optimizer, float, Optional[str]]:\n",
    "    if version is None:\n",
    "        return model, optimizer, epsilon, None\n",
    "    version_path = os.path.join(path, version)\n",
    "\n",
    "    model_path = os.path.join(version_path, f\"pong-policy-net-{episode}.pt\")\n",
    "    optimizer_path = os.path.join(version_path, f\"pong-optimizer-{episode}.pt\")\n",
    "    epsilon_path = os.path.join(version_path, f\"epsilon-{episode}.json\")\n",
    "\n",
    "    if any([not os.path.exists(p) for p in [model_path, optimizer_path, epsilon_path]]):\n",
    "        return model, optimizer, epsilon\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    optimizer.load_state_dict(torch.load(optimizer_path))\n",
    "    with open(epsilon_path, \"r\") as f:\n",
    "        epsilon = json.loads(f.read())[\"epsilon\"]\n",
    "\n",
    "    return model, optimizer, epsilon, version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(\n",
    "    env,\n",
    "    memory: deque,\n",
    "    policy_net: DualDQN,\n",
    "    target_net: DualDQN,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    epsilon: float,\n",
    "    version=None,\n",
    "):\n",
    "    num_episodes = 1000\n",
    "    epsilon = epsilon\n",
    "    version = f'pong_dqn_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    writer = SummaryWriter(\"runs/\" + version)\n",
    "    global_step = 0\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        state = torch.stack([state] * 4, axis=0)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(\n",
    "                state=state,\n",
    "                epsilon=epsilon,\n",
    "                action_space=env.action_space.n,\n",
    "                policy_net=policy_net,\n",
    "            )\n",
    "            next_obs, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            total_reward += reward\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            next_state = torch.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            loss = optimize_model(\n",
    "                memory=memory,\n",
    "                policy_net=policy_net,\n",
    "                target_net=target_net,\n",
    "                optimizer=optimizer,\n",
    "            )\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "\n",
    "            if steps % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if global_step % 1000 == 0:\n",
    "                if epsilon > EPSILON_END:\n",
    "                    epsilon *= EPSILON_DECAY\n",
    "        if episode % 10 == 0:\n",
    "            save_model_and_hyperparameter(\n",
    "                model=policy_net,\n",
    "                optimizer=optimizer,\n",
    "                epsilon=epsilon,\n",
    "                episode=episode,\n",
    "                version=version,\n",
    "            )\n",
    "        writer.add_scalar(\"Total Loss\", total_loss, episode)\n",
    "        writer.add_scalar(\"Steps\", steps, episode)\n",
    "        writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gym setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(ENV)\n",
    "memory = deque(maxlen=MAX_MEMORY_SIZE)\n",
    "epsilon = EPSILON_START"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize policy and target network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = DualDQN(env.action_space.n).to(device)\n",
    "target_net = DualDQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model and optimizer from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = 0\n",
    "version = None\n",
    "# version = get_latest_version()\n",
    "policy_net, optimizer, epsilon, version = load_model_and_hyperparameter(\n",
    "    model=policy_net,\n",
    "    optimizer=optimizer,\n",
    "    epsilon=epsilon,\n",
    "    episode=episode,\n",
    "    version=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train(\n",
    "    env=env,\n",
    "    memory=memory,\n",
    "    policy_net=policy_net,\n",
    "    target_net=target_net,\n",
    "    optimizer=optimizer,\n",
    "    epsilon=epsilon,\n",
    "    version=version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# obs = env.step(0)\n",
    "# obs = preprocess_observation(obs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
