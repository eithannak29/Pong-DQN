{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ale_py\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"PongDeterministic-v4\"\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.97\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 0.99\n",
    "TARGET_UPDATE = 1000\n",
    "MAX_MEMORY_SIZE = 25000\n",
    "MIN_MEMORY_SIZE = 24000\n",
    "LEARNING_RATE = 0.00025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DualDQN(nn.Module):\n",
    "    def __init__(self, action_space):\n",
    "        super(DualDQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.advantage_stream = nn.Linear(1536, 128)\n",
    "        self.advantage_stream2 = nn.Linear(128, action_space)\n",
    "        self.value_stream = nn.Linear(1536, 128)\n",
    "        self.value_stream2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        value = self.value_stream2(F.leaky_relu(self.value_stream(x)))\n",
    "        advantage = self.advantage_stream2(F.leaky_relu(self.advantage_stream(x)))\n",
    "        q_values = value + (advantage - advantage.mean())\n",
    "\n",
    "        return q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.register_envs(ale_py)\n",
    "env = gym.make(ENV)\n",
    "policy_net = DualDQN(env.action_space.n).to(device)\n",
    "target_net = DualDQN(env.action_space.n).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "memory = deque(maxlen=MAX_MEMORY_SIZE)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(image):\n",
    "    target_h = 80\n",
    "    target_w = 64\n",
    "    crop_dim = [20, image.shape[0], 0, image.shape[1]]\n",
    "    frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    frame = frame[crop_dim[0] : crop_dim[1], crop_dim[2] : crop_dim[3]]\n",
    "    frame = cv2.resize(frame, (target_w, target_h))\n",
    "    frame = frame.reshape(target_w, target_h) / 255\n",
    "    return torch.tensor(frame, dtype=torch.float32).to(device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < MIN_MEMORY_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(\n",
    "        *transitions\n",
    "    )\n",
    "\n",
    "    batch_state = torch.cat(batch_state).unflatten(0, (BATCH_SIZE, -1)).to(device)\n",
    "    batch_action = torch.tensor(batch_action).to(device)\n",
    "    batch_reward = torch.tensor(batch_reward).to(device)\n",
    "    batch_next_state = (\n",
    "        torch.cat(batch_next_state).unflatten(0, (BATCH_SIZE, -1)).to(device)\n",
    "    )\n",
    "    batch_done = torch.tensor(batch_done, dtype=torch.bool).to(device)\n",
    "\n",
    "    current_q_values = (\n",
    "        policy_net(batch_state).gather(1, batch_action.unsqueeze(1)).squeeze(1)\n",
    "    )\n",
    "    next_q_values = policy_net(batch_next_state)\n",
    "    next_target_q_values = (\n",
    "        target_net(batch_next_state)\n",
    "        .gather(1, next_q_values.max(1)[1].unsqueeze(1))\n",
    "        .squeeze(1)\n",
    "    )\n",
    "    expected_q_values = batch_reward + (GAMMA * next_target_q_values) * (~batch_done)\n",
    "\n",
    "    loss = nn.SmoothL1Loss()(current_q_values, expected_q_values.detach())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, epsilon, action_space):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randrange(action_space)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state = state.unsqueeze(0)\n",
    "            return torch.argmax(policy_net(state)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    num_episodes = 1000\n",
    "    epsilon = EPSILON_START\n",
    "    writer = SummaryWriter(\n",
    "        f'runs/pong_dqn_{datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n",
    "    )\n",
    "    global_step = 0\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training Episodes\"):\n",
    "        obs, _ = env.reset()\n",
    "        state = preprocess_observation(obs)\n",
    "        state = torch.stack([state] * 4, axis=0)\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(state, epsilon, env.action_space.n)\n",
    "            next_obs, reward, done, truncated, _ = env.step(action)\n",
    "            done = done or truncated\n",
    "            total_reward += reward\n",
    "            next_state = preprocess_observation(next_obs)\n",
    "            next_state = torch.stack((next_state, state[0], state[1], state[2]))\n",
    "\n",
    "            memory.append((state, action, reward, next_state, done))\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            global_step += 1\n",
    "\n",
    "            loss = optimize_model()\n",
    "            if loss is not None:\n",
    "                total_loss += loss\n",
    "\n",
    "            if steps % TARGET_UPDATE == 0:\n",
    "                target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "            if global_step % 1000 == 0:\n",
    "                if epsilon > EPSILON_END:\n",
    "                    epsilon *= EPSILON_DECAY\n",
    "\n",
    "        writer.add_scalar(\"Total Loss\", total_loss, episode)\n",
    "        writer.add_scalar(\"Steps\", steps, episode)\n",
    "        writer.add_scalar(\"Epsilon\", epsilon, episode)\n",
    "        writer.add_scalar(\"Total Reward\", total_reward, episode)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.reset()\n",
    "# obs = env.step(0)\n",
    "# obs = preprocess_observation(obs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
